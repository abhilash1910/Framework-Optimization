{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5KIXyLph17q",
    "outputId": "69ef9a03-27e4-4953-a50c-7ae3d540008d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized process group with 1 ranks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-c4c28eee579b>:63: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  0 , epoch  0 :  1.3033309768257872\n",
      "Rank  0 , epoch  1 :  0.5454973846610421\n",
      "Rank  0 , epoch  2 :  0.4244839232931259\n",
      "Rank  0 , epoch  3 :  0.3598236723113924\n",
      "Rank  0 , epoch  4 :  0.3233929915405286\n",
      "Rank  0 , epoch  5 :  0.2905611474948651\n",
      "Rank  0 , epoch  6 :  0.26836184588576684\n",
      "Rank  0 , epoch  7 :  0.250192261819265\n",
      "Rank  0 , epoch  8 :  0.23438266896680474\n",
      "Rank  0 , epoch  9 :  0.22443408579396795\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.multiprocessing import Process\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from math import ceil\n",
    "from random import Random\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import os\n",
    "\n",
    "def init_process_group(rank, size, backend='nccl'):\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    assert (size==dist.get_world_size()),f\"{size} and {dist.get_world_size()} does not match\"\n",
    "    return dist\n",
    "\n",
    "def all_reduce(tensor,op):\n",
    "  dist.all_reduce(tensor,op)\n",
    "\n",
    "def reduce(tensor,op):\n",
    "  dist.reduce(tensor,op)\n",
    "\n",
    "def average_grads_allreduce(model):\n",
    "  for param in model.parameters():\n",
    "    reduce(param.grad.data,op=dist.ReduceOp.SUM)\n",
    "    param.grad.data/=float(dist.get_world_size())\n",
    "\n",
    "def set_seed(val):\n",
    "  torch.manual_seed(val)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(val)\n",
    "\n",
    "def set_cuda_device(dist):\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(dist.get_rank())\n",
    "\n",
    "\n",
    "dist=init_process_group(0,1)\n",
    "print(f\"Initialized process group with {dist.get_world_size()} ranks\")\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Network architecture. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "class Partition(object):\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "class DataPartitioner(object):\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "def partition_dataset():\n",
    "    dataset = datasets.MNIST(\n",
    "        './data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "        ]))\n",
    "    size = dist.get_world_size()\n",
    "    bsz = 128 // size\n",
    "    partition_sizes = [1.0 / size for _ in range(size)]\n",
    "    partition = DataPartitioner(dataset, partition_sizes)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    train_set = torch.utils.data.DataLoader(\n",
    "        partition, batch_size=bsz, shuffle=True)\n",
    "    return train_set, bsz\n",
    "\n",
    "def run(dist):\n",
    "  set_seed(1234)\n",
    "  train_set, bsz = partition_dataset()\n",
    "  model = Net()\n",
    "  if torch.cuda.is_available():\n",
    "      set_cuda_device(dist)\n",
    "  if dist.get_world_size()>1:\n",
    "      model= DistributedDataParallel(model,device_ids=dist.get_rank(),output_device=dist.get_rank())\n",
    "  else:\n",
    "      model=model.to(device='cuda:0')\n",
    "  optimizer = torch.optim.SGD(model.parameters(),\n",
    "                        lr=0.01, momentum=0.5)\n",
    "\n",
    "  num_batches = ceil(len(train_set.dataset) / float(bsz))\n",
    "  for epoch in range(10):\n",
    "      epoch_loss = 0.0\n",
    "      for data, target in train_set:\n",
    "          data, target= data.to(device='cuda'),target.to(device='cuda')\n",
    "          optimizer.zero_grad()\n",
    "          output = model(data)\n",
    "          loss = F.nll_loss(output, target)\n",
    "          epoch_loss += loss.item()\n",
    "          loss.backward()\n",
    "          average_grads_allreduce(model)\n",
    "          optimizer.step()\n",
    "      print('Rank ', dist.get_rank(), ', epoch ',\n",
    "            epoch, ': ', epoch_loss / num_batches)\n",
    "run(dist)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
